image:
  repository: tootsuite/mastodon
  pullPolicy: IfNotPresent
  tag: "v4.0.2"

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: { }

podAnnotations: { }

podSecurityContext:
  fsGroup: 991
  runAsGroup: 991
  runAsUser: 991

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  runAsNonRoot: true
  runAsUser: 991

configuration:
  database:
    host: tootcommunity-db-pooler-rw
    name: app
    poolsize: 25
    port: 5432
    sslmode: prefer
    credentialSecret:
      name: tootcommunity-db-app
      usernameKey: username
      passwordKey: password
  databaseReplicas:
    enabled: false
    hosts:
      - tootcommunity-db-pooler-ro
  defaultLocale: en
  localDomain: toot.community
  redis:
    host: redis-haproxy.haproxy.svc.cluster.local
    port: 6379
    database: 0
  s3:
    aliasHost: static.toot.community
    bucket: static-toot-community
    endpoint: https://0ae89288096e9795bb1a5d327d89aec5.r2.cloudflarestorage.com
    hostname: 0ae89288096e9795bb1a5d327d89aec5.r2.cloudflarestorage.com
    region: auto
    protocol: https
  smtp:
    authMethod: plain
    caFile: /etc/ssl/certs/ca-certificates.crt
    deliveryMethod: smtp
    enableStarttlsAuto: true
    opensslVerifyMode: none
    port: 587
    domain: ses.toot.community
    fromAddress: notifications@ses.toot.community
    smtpServer: email-smtp.eu-west-1.amazonaws.com

web:
  domain: toot.community
  concurrency: 2
  maxThreads: 6
  resources:
    requests:
      cpu: 750m
      memory: 750Mi
  startupProbe:
    httpGet:
      port: http
      path: /health
    periodSeconds: 3
    failureThreshold: 30
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /health
      port: http
      scheme: HTTP
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  # the primary db deployment will capture all /oauth & verify credential requests to workaround
  # any race conditions during db replication
  primaryDatabaseDeployment:
    enabled: false
    replicaCount: 2
    resources:
      requests:
        cpu: 50m
        memory: 200Mi

statistics:
  enabled: true
  address: exporter-prometheus-statsd-exporter.monitoring.svc.cluster.local:9125

translations:
  enabled: true
  endpoint: https://translate.universeodon.com

streaming:
  replicaCount: 3
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /api/v1/streaming/health
      port: streaming
      scheme: HTTP
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 10
  startupProbe:
    httpGet:
      port: streaming
      path: /api/v1/streaming/health
    periodSeconds: 3
    failureThreshold: 30
  resources:
    requests:
      cpu: 25m
      memory: 150Mi
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

sidekiq:
  nodeSelector:
    doks.digitalocean.com/node-pool: pool-sidekiq-3
  tolerations:
    - effect: NoSchedule
      key: limit
      operator: Equal
      value: sidekiq
  workers:
    - name: default
      concurrency: 25
      queues:
        - default
      resources:
        requests:
          cpu: 500m
          memory: 350Mi
    - name: ingress
      concurrency: 25
      queues:
        - ingress
      resources:
        requests:
          cpu: 750m
          memory: 500Mi
    - name: pull
      concurrency: 25
      queues:
        - pull
      resources:
        requests:
          cpu: 500m
          memory: 350Mi
    - name: push
      concurrency: 25
      queues:
        - push
      resources:
        requests:
          cpu: 500m
          memory: 350Mi
    - name: mailers
      concurrency: 5
      queues:
        - mailers
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
    - name: scheduler
      concurrency: 5
      queues:
        - scheduler
      resources:
        requests:
          cpu: 100m
          memory: 100Mi

ingress:
  ingressClassName: nginx
  host: toot.community
  tls:
    - hosts:
        - toot.community
      secretName: cf-origin-certificate-tls

jobs:
  annotations: {}
  cleanupMedia:
    enabled: true
    schedule: "0 5 * * *"
    days: 7
    resources:
      requests:
        cpu: 1000m
  removePreviewCards:
    enabled: true
    schedule: "0 5 * * 1"
    resources:
      requests:
        cpu: 500m
        memory: 400Mi
    days: 180

onePassword:
    ## Contents:
    ## - tls.crt: <content>
    ## - tls.key: <content>
  - name: cf-origin-certificate-tls
    itemPath: vaults/dhwqy3tub7zdwf2zj6cv4lprlu/items/hpm4njgmdollluy5b4isc2zpgq
    ## It holds the following keys:
    ## - AWS_ACCESS_KEY_ID
    ## - AWS_SECRET_ACCESS_KEY
    ## - DB_PASS
    ## - OTP_SECRET
    ## - REDIS_PASSWORD
    ## - SECRET_KEY_BASE
    ## - SMTP_LOGIN
    ## - SMTP_PASSWORD
    ## - VAPID_PRIVATE_KEY
    ## - VAPID_PUBLIC_KEY
  - suffix: secrets-env
    itemPath: vaults/dhwqy3tub7zdwf2zj6cv4lprlu/items/oezhcrluaumwwkrwulo62wnmhe
    ## Contents:
    ## - ACCESS_KEY_ID: <content>
    ## - ACCESS_SECRET_KEY: <content>
  - name: db-backup-toot-community-creds
    itemPath: vaults/dhwqy3tub7zdwf2zj6cv4lprlu/items/2pry55mb7zpdtbssb4iohrc3iy

keda:
  prometheusAddress: http://prometheus-server.monitoring.svc.cluster.local
  web:
    enabled: true
    cooldownPeriod: 30
    maxReplicaCount: 40
    minReplicaCount: 2
    pollingInterval: 15
    fallback:
      enabled: true
      replicas: 5
      failureThreshold: 4 # allows prometheus to be down for (4 * 15) seconds before going to fallback
    ignoreNullValues: false
    threshold: 25
    advanced:
      horizontalPodAutoscalerConfig:
        behavior:
          scaleDown:
            policies:
              - type: Percent
                value: 25
                periodSeconds: 60
          scaleUp:
            stabilizationWindowSeconds: 60
    memoryUtilizationScaling:
      enabled: false
      targetAverageUtilization: 125
  workers:
    enabled: true
    cooldownPeriod: 300
    maxReplicaCount: 10
    minReplicaCount: 1
    pollingInterval: 10
    ignoreNullValues: false
    fallback:
      enabled: true
      replicas: 1
      failureThreshold: 4
    advanced:
      horizontalPodAutoscalerConfig:
        behavior:
          scaleDown:
            stabilizationWindowSeconds: 300
            policies:
              - type: Pods
                value: 1
                periodSeconds: 60
          scaleUp:
            stabilizationWindowSeconds: 30
            policies:
              - type: Pods
                value: 1
                periodSeconds: 30
    queues:
      - name: default
        threshold: 1
        queue: default
      - name: ingress
        threshold: 5
        queue: ingress
      - name: pull
        threshold: 300
        queue: pull
      - name: push
        threshold: 5
        queue: push
